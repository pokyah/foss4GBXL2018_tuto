---
title: "spagrometeoR"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

<style>
.tutorial-exercise-output pre{
    color:#43d615;
    background-color:black;
    border-radius: 10px;
    padding: 20px;}
}

.ace_editor{
    font-size:16px !important;
}
</style>

```{r setup, include=FALSE}

# devtools::install_github("r-spatial/sf")
# devtools::install_github("hadley/devtools")

library(devtools)
library(learnr)
library(rgdal)
library(sp)
library(raster)
library(sf)
library(rgeos)
library(fontawesome)
library(leaflet)
library(mlr)
library(dplyr)
library(ggplot2)
library(FNN)
library(agrometAPI)
library(elevatr)
library(rnaturalearth)
library(rnaturalearthhires)

# downloading data
rawdata = agrometAPI::get_data(dfrom = (Sys.Date() - 1))
rawdata = agrometAPI::type_data(rawdata)

# filtering 
mydataset = rawdata %>%
  dplyr::filter(!is.na(mtime)) %>%
  dplyr::filter(sid != 38 & sid != 41) %>%
  dplyr::filter(!is.na(from)) %>%
  dplyr::filter(!is.na(to)) %>%
  dplyr::filter(poste != "China") %>%
  dplyr::filter(!type_name %in% c("PS2000","PESSL","BODATA","Sencrop","netdl1000","SYNOP")) %>%
  dplyr::select(c(sid, poste, longitude, latitude, altitude, mtime, tsa))

# making it spatial
mydataset = sf::st_as_sf(mydataset, 
  coords = c("longitude", "latitude"),
  crs = 4326)
sf::st_crs(mydataset)


# downloading admin boundaries of Belgium
belgium = sf::st_as_sf((ne_states(country = 'belgium')))
wallonia = belgium %>% dplyr::filter(region == "Walloon")
class(wallonia)
# checking CRS
sf::st_crs(wallonia)

# downloading DEM
# for correspondance between zoom & res : https://mapzen.com/documentation/terrain-tiles/data-sources/#what-is-the-ground-resolution
elevation = elevatr::get_elev_raster(as(wallonia, "Spatial"), z = 5, src = "aws")
class(elevation)
# checking CRS
raster::crs(elevation, asText = TRUE)

# croping DEM to wallonia
elevation = raster::mask(elevation, as(wallonia, "Spatial"))
# ploting the elevation raster
plot(elevation)

# projected for grid resolution defined in meters
wallonia = sf::st_transform(wallonia, crs = 3812)

#  cropping
grid = sf::st_sf(sf::st_make_grid(x = wallonia,  cellsize = 5000, what = "centers"))
grid = sf::st_intersection(grid, wallonia)
grid = sf::st_transform(grid, crs = 4326)
wallonia = sf::st_transform(wallonia, crs = 4326)
plot(grid)

# elevation ==> grid extraction 
extracted <- raster::extract(
  elevation,
  as(grid,"Spatial"),
  fun = mean,
  na.rm = TRUE,
  df = TRUE
)
colnames(extracted) = c("ID", "altitude")

grid$altitude = extracted$altitude
grid = dplyr::select(grid, altitude)
grid = dplyr::filter(grid, !is.na(altitude))
plot(grid)
sf::st_crs(grid)


# leaflet
 elevation.pal <- colorNumeric(reverse = TRUE, "RdYlGn", values(elevation),
  na.color = "transparent")
temperature.pal <- colorNumeric(reverse = TRUE, "RdBu", domain=mydataset$tsa,
  na.color = "transparent")
responsiveness = "\'<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\'"

map <- leaflet() %>% 
     addProviderTiles(
         providers$OpenStreetMap.BlackAndWhite, group = "B&W") %>%
     addProviderTiles(
         providers$Esri.WorldImagery, group = "Satelitte") %>%
     addRasterImage(
         elevation, group = "Elevation", colors = elevation.pal, opacity = 0.8) %>%
     addPolygons(
         data = wallonia, group = "Admin", color = "#444444", weight = 1, smoothFactor = 0.5,
         opacity = 1, fillOpacity = 0.1, fillColor = "grey") %>%
     addCircleMarkers(
         data = mydataset,
         group = "Stations",
         color = ~temperature.pal(tsa),
         stroke = FALSE,
        fillOpacity = 0.8,
         label = ~htmltools::htmlEscape(as.character(tsa))) %>%
    addCircleMarkers(
        data = grid,
        group = "Grid",
        radius = 2,
        color = "blue",
        stroke = TRUE, fillOpacity = 1) %>%
    addLegend(
      values = values(elevation), group = "Elevation",
      position = "bottomright", pal = elevation.pal,
      title = "Elevation (m)") %>%
     addLayersControl(
         baseGroups = c("B&W", "Satelitte"),
         overlayGroups = c("Elevation", "Admin", "Stations", "Grid"),
         options = layersControlOptions(collapsed = TRUE)
     ) %>%
     hideGroup(c("Slope", "Aspect")) %>%
     addEasyButton(easyButton(
         icon = "fa-crosshairs", title = "Locate Me",
         onClick = JS("function(btn, map){ map.locate({setView: true}); }"))) %>%
     htmlwidgets::onRender(paste0("
       function(el, x) {
       $('head').append(",responsiveness,");
       }"))
map


# extract the lon and lat from the sf geometry
coords = data.frame(st_coordinates(mydataset))
# attributing our original dataset to another var (to avoid overwriting)
ourTask = mydataset
# converting our dataset from sf to simple df
st_geometry(ourTask) <- NULL
# joining the coords
ourTask = dplyr::bind_cols(ourTask, coords)
# Dropping the non-explanatory features
ourTask = dplyr::select(ourTask, -c(sid, poste, mtime))
# defining our taks
ourTask = mlr::makeRegrTask(id = "FOSS4G_example", data = ourTask, target = "tsa")
# checking our data
head(mlr::getTaskData(ourTask))

# Defining our learners
ourLearners = mlr::makeLearners(
  cls = c("regr.lm", "regr.fnn", "regr.nnet"),
  ids = c("linearRegression", "Fast Nearest Neighbours", "Neural Network")
)

# Defining our learners
ourResamplingStrategy = mlr::makeResampleDesc("LOO")

# performing the benchmark of our learners on our task
ourbenchmark = mlr::benchmark(
  learners = ourLearners,
  tasks = ourTask,
  resamplings = ourResamplingStrategy,
  measures = list(rmse)
)

performances = mlr::getBMRAggrPerformances(bmr = ourbenchmark, as.df = TRUE)
performances

# Vizualizing the benchamrk result
library(ggplot2)
plotBMRBoxplots(
  bmr = ourbenchmark,
  measure = rmse,
  order.lrns = mlr::getBMRLearnerIds(ourbenchmark)) +
  aes(color = learner.id)

# extract the lon and lat from the sf geometry
coords = data.frame(st_coordinates(grid))
# attributing our original dataset to another var (to avoid overwriting)
ourPredictionGrid = grid
# converting our dataset from sf to simple df
st_geometry(ourPredictionGrid) <- NULL
# joining the coords
ourPredictionGrid = dplyr::bind_cols(ourPredictionGrid, coords)

# training the neural net on the dataset
ourModel = mlr::train(
  learner = mlr::getBMRLearners(bmr = ourbenchmark)[[1]],
  task = ourTask)

# using our model to make the prediction
ourPrediction = predict(
  object = ourModel,
  newdata = ourPredictionGrid
)$data

# injecting the predicted values in the prediction grid
ourPredictedGrid = dplyr::bind_cols(ourPredictionGrid, ourPrediction)

# making the predicted grid a spatial object again
ourPredictedGrid = sf::st_as_sf(ourPredictedGrid, coords = c("X", "Y"), crs = 4326)
plot(ourPredictedGrid)

# injecting data in polygons for better rendering
# https://r-spatial.github.io/sf/reference/st_make_grid.html

sfgrid = st_sf(sf::st_make_grid(x = sf::st_transform(wallonia, 3812),  cellsize = 5000, what = "polygons"))
ourPredictedGrid = sf::st_transform(ourPredictedGrid, crs = 3812)
ourPredictedGrid = sf::st_join(sfgrid, ourPredictedGrid)
ourPredictedGrid = ourPredictedGrid %>%
  dplyr::filter(!is.na(response))

# back to 4326 for leaflet
ourPredictedGrid = sf::st_transform(ourPredictedGrid, 4326)

# adding to our map
map2 = map %>% 
  addPolygons(
    data = ourPredictedGrid,
    group = "Predictions",
    color = ~temperature.pal(response),
    stroke = FALSE,
    fillOpacity = 0.9,
    label = ~htmltools::htmlEscape(as.character(response))) %>%
  addLegend(
    values = ourPredictedGrid$response,
    group = "Predictions",
    position = "bottomleft", pal = temperature.pal,
    title = "predicted T (°C)") %>%
  addLayersControl(
         baseGroups = c("B&W", "Satelitte"),
         overlayGroups = c("Elevation", "Admin", "Grid", "Predictions", "Stations"),
         options = layersControlOptions(collapsed = TRUE)
     )
map2
```

## WELCOME ! 

Welcome to this Session! 

> Why and How to use R as an opensource GIS : The AGROMET project usecase

presented by *Thomas Goossens*

<center>
`r fontawesome::fa("linkedin", height = "25px", fill = "#75aadb")` 
`r fontawesome::fa("github", height = "25px", fill = "#75aadb")`  
`r fontawesome::fa("envelope", height = "25px", fill = "#75aadb")`
</center>

```{r foss4gbelogo, echo = FALSE, out.width = "35%"}
knitr::include_graphics("images/foss4gbe.svg")
```

```{r crawlogo, echo = FALSE, out.width = "35%"}
knitr::include_graphics("images/craw_fr.png")
```

## PROJECT PRESENTATION

> A very short introduction to the project

### What? 

`r fontawesome::fa("bullseye", height = "50px", fill = "#75aadb")`
Providing __hourly__ gridded weather data @ __1km² resolution__ for Wallonia

### Why? 

`r fontawesome::fa("leaf", height = "50px", fill = "#75aadb")`

Feeding decision tools for agricultural warning systems based on crop monitoring models ([EU directive for Sustainable use of pesticides](https://ec.europa.eu/food/plant/pesticides/sustainable_use_pesticides_en))

### How? 

`r fontawesome::fa("th", height = "50px", fill = "#75aadb")`
__spatializing__ data from [PAMESEB](https://www.pameseb.be/) Automatic Weather Station Network

## GIS BUILDING BLOCKS

### DATA

`r fa("cubes" , height = "50px", fill = "#75aadb")`

* Weather data from the stations
* explanatory variables (DEM, land cover, etc)
* interpolation grid
* map backgrounds

### TOOLS

`r fa("wrench" , height = "50px", fill = "#75aadb")`

* API query
* Data manipulation
* interpolation algorithms (linear regression, ANN, kriging)
* algorithms output benchmarking
* dataviz/mapping tools

## WHY R ?

`r fa("r-project" , height = "50px", fill = "#75aadb")`
* Already used by our weather specialist partners : 
[RMI](www/Poster_Eumetnet_2017.pdf) + [KNMI](http://dailymeteo.org/Sluiter2014)
* [Increased popularity among tech companies](https://thenextweb.com/offers/2018/03/28/tech-giants-are-harnessing-r-programming-learn-it-and-get-hired-with-this-complete-training-bundle/)
* [Impressive growth](https://stackoverflow.blog/2017/10/10/impressive-growth-r/) and active community

## CODE LIBRARIES

> Libraries are packaged thematic R-softwares containing predefined sets of functions to help you save code

```{r librariesLoading, exercise = TRUE}
library(devtools)
library(learnr)
library(rgdal)
library(sp)
library(raster)
library(sf)
library(rgeos)
library(fontawesome)
library(leaflet)
library(mlr)
library(dplyr)
library(ggplot2)
library(FNN)
library(agrometAPI)
library(elevatr)
library(rnaturalearth)
library(naturalearthhires)
library(rnaturalearthhires)
```

## DATA ACQUISITION

> We can use our various libraries to acquire the required datasets (DEM, boundaries, weather stations data, etc...)

`r fa("r-project")` libraries are available on CRAN and `r fa("github")`

### AGROMET WEATHER STATIONS DATA

> We can get the temperature data from our stations using the `agrometAPI` library

1. Calling the API 

```{r agrometAPIget, exercise=TRUE}
# getting data from API (requires a token)
rawdata = agrometAPI::get_data(dfrom = (Sys.Date() - 1))
rawdata = agrometAPI::type_data(rawdata)

# Let's see how it looks
rawdata
```

2. Filtering data

```{r agrometAPIfilter, exercise=TRUE}
# Let's Filter it using dplyr
mydataset = rawdata %>%
  dplyr::filter(!is.na(mtime)) %>%
  dplyr::filter(sid != 38 & sid != 41) %>%
  dplyr::filter(!is.na(from)) %>%
  dplyr::filter(!is.na(to)) %>%
  dplyr::filter(poste != "China") %>%
  dplyr::filter(!type_name %in% c("PS2000","PESSL","BODATA","Sencrop","netdl1000","SYNOP")) %>%
  dplyr::select(c(sid, poste, longitude, latitude, altitude, mtime, tsa))

# Let's see how it looks
mydataset
```

3. Making the data spatial (__! Coordinate Reference System ([CRS](https://epsg.io/))__)

```{r agrometAPIspatial, exercise=TRUE}
# making it spatial
mydataset = sf::st_as_sf(mydataset, 
  coords = c("longitude", "latitude"),
  crs = 4326)

# outputing the CRS to the console
sf::st_crs(mydataset)
```

### ADMIN BOUNDARIES

> Many R libraries provide the ability to get admin boundaries. We use `rnaturalearth`

1. Downloading Belgium admin bondaries 
```{r adminGet, exercise=TRUE}
# downloading admin boundaries of Belgium
belgium = sf::st_as_sf((rnaturalearth::ne_states(country = 'belgium')))
# inspecting the data
belgium
```

2. Filtering to only keep Wallonia

```{r adminFilter, exercise=TRUE}
# Filtering Wallonia data
wallonia = belgium %>% dplyr::filter(region == "Walloon")

# Checking the Coordinate Reference system
st_crs(wallonia)
```

### ELEVATION DATA - DEM RASTER

> Again, many libraries provide ways to download such data. We use `elevatr`

1. Downloading data.

Raster resolutioncontrolled by `z` param. See [package documentation](https://cran.r-project.org/web/packages/elevatr/vignettes/introduction_to_elevatr.html#get_raster_elevation_data)

```{r elevationGet, exercise=TRUE}
# downloading DEM data and storing it in an object. Z = 5 ==> about 3km² resolution
elevation = elevatr::get_elev_raster(as(wallonia, "Spatial"), z = 5, src = "aws")
class(elevation)
plot(elevation)
# checking CRS
raster::crs(elevation, asText = TRUE)
```

2. Cropping with Wallonia borders

```{r elevationCrop, exercise=TRUE}
# masking
elevation = raster::mask(elevation, as(wallonia, "Spatial"))
# ploting the elevation raster
plot(elevation)
```

### INTERPOLATION GRID

> Grid is quickly built with new `sf` library

1. Reprojecting the extent of our interest zone from __geographic__ to __projected__ CRS. See [this](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf) cheatsheet.

```{r gridTrans, exercise=TRUE}
# projected for grid resolution defined in meters
wallonia = sf::st_transform(wallonia, crs = 3812)
```

2. Building the grid

```{r gridBuild, exercise=TRUE}
# building the grid at 5 km² resolution
grid = sf::st_sf(sf::st_make_grid(x = wallonia,  cellsize = 5000, what = "centers"))
# limit it to Wallonia and not full extent
grid = sf::st_intersection(grid, wallonia)
# visualizing 
plot(grid)
```

3. Projecting back to __geographic__ CRS

```{r gridReproj, exercise=TRUE}
grid = sf::st_transform(grid, crs = 4326)
wallonia = sf::st_transform(grid, crs = 4326)
```

### FEEDING THE GRID WITH EXPLANATORY DATA

> Injecting explanatory variables (limited to elevation for this example) in the grid. 

1. Extracting raster data at the locations of grid points

```{r feedingExtract, exercise=TRUE}
# extracting
extracted = raster::extract(
  elevation,
  as(grid,"Spatial"),
  fun = mean,
  na.rm = TRUE,
  df = TRUE
)
# inspecting
extracted
# renaming columns
colnames(extracted) = c("ID", "altitude")
```

2. Injecting the extracted dataframe output into the grid
```{r feedingInject, exercise=TRUE}
# injecting
grid$altitude = extracted$altitude
# keeping only the altitude data
grid = dplyr::select(grid, altitude)
# removing locations where altitude is NA
grid = dplyr::filter(grid, !is.na(altitude))
# visualizing
plot(grid)
# checking CRS
sf::st_crs(grid)
```

## DATA VISUALIZATION

### LEAFLET MAP

> Making our spatial data intelligible by mapping it using `leaflet`

1. preparing the color palettes and settings for mobile phone responsiveness

```{r, leafletSettings, exercise = TRUE}
elevation.pal <- colorNumeric(reverse = TRUE, "RdYlGn", values(elevation),
  na.color = "transparent")
temperature.pal <- colorNumeric(reverse = TRUE, "RdBu", domain=mydataset$tsa,
  na.color = "transparent")
responsiveness = "\'<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\'"
```

2. mapping the various datasets

```{r, leafletMap, exercise = TRUE}
map <- leaflet() %>% 
     addProviderTiles(
         providers$OpenStreetMap.BlackAndWhite, group = "B&W") %>%
     addProviderTiles(
         providers$Esri.WorldImagery, group = "Satelitte") %>%
     addRasterImage(
         elevation, group = "Elevation", colors = elevation.pal, opacity = 0.8) %>%
     addPolygons(
         data = wallonia, group = "Admin", color = "#444444", weight = 1, smoothFactor = 0.5,
         opacity = 1, fillOpacity = 0.1, fillColor = "grey") %>%
     addCircleMarkers(
         data = mydataset,
         group = "Stations",
         color = ~temperature.pal(tsa),
         stroke = FALSE,
        fillOpacity = 0.8,
         label = ~htmltools::htmlEscape(as.character(tsa))) %>%
    addCircleMarkers(
        data = grid,
        group = "Grid",
        radius = 2,
        color = "blue",
        stroke = TRUE, fillOpacity = 1) %>%
    addLegend(
      values = values(elevation), group = "Elevation",
      position = "bottomright", pal = elevation.pal,
      title = "Elevation (m)") %>%
     addLayersControl(
         baseGroups = c("B&W", "Satelitte"),
         overlayGroups = c("Elevation", "Admin", "Stations", "Grid"),
         options = layersControlOptions(collapsed = TRUE)
     ) %>%
     hideGroup(c("Slope", "Aspect")) %>%
     addEasyButton(easyButton(
         icon = "fa-crosshairs", title = "Locate Me",
         onClick = JS("function(btn, map){ map.locate({setView: true}); }"))) %>%
     htmlwidgets::onRender(paste0("
       function(el, x) {
       $('head').append(",responsiveness,");
       }"))
map
```

## INTERPOLATION

> Spatialization or spatial interpolation creates a continuous surface from values measured at discrete locations to __predict__ values at any location in the interest zone with the __best accuracy__.

### 2 approaches 

> To predict values at any location : 

1. ~~physical atmospherical models~~ (not straight forward to develop an explicit physical model describing how the output data can be derived from the input data)

2. __supervised machine learning regression algorithms__ that given a set of continuous data, find the best relationship that represents the set of continuous data (common approach largely discussed in the academic litterature)

### Supervised Machine learning

> We will go through a very simple example of machine learning usecase

### Machine Learning definition

From machinelearningmastery.com :

> Supervised learning is where you have input variables (x) and an output variable (Y) and you use an algorithm to learn the mapping function from the input to the output : Y = f(X)
The goal is to approximate the mapping function so well that when you have new input data (x), you can predict the output variables (Y) for that data.
It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process

### MLR library

[](https://mlr-org.github.io/mlr/reference/figures/logo_navbar.png)
[go to mlr website for full details](https://mlr-org.github.io/mlr/index.html)

`mlr` is a `r fa("r-project")` library that offers a standardized interface for all its machine learning algorithms. 

### the idea

* For each hourly set of temperature records (30 stations) ...
* run a benchmark experiment where different regression learning algorithms are used to learn ...
* from various regression tasks (i.e. datasets with different combinations of explanatory variables + the target weather parameter) ...
* with the aim to compare and rank the performances of combinations of algorithm + used explanatory variables using a cross validation resampling strategy (LOOCV)

### Defining our ML task
```{r taskMLR, exercise = TRUE}
# loading the mlr library
library(mlr)
# extract the lon and lat from the sf geometry
coords = data.frame(st_coordinates(mydataset))
# attributing our original dataset to another var (to avoid overwriting)
ourTask = mydataset
# converting our dataset from sf to simple df
st_geometry(ourTask) <- NULL
# joining the coords
ourTask = dplyr::bind_cols(ourTask, coords)
# Dropping the non-explanatory features
ourTask = dplyr::select(ourTask, -c(sid, poste, mtime))
# defining our taks
ourTask = mlr::makeRegrTask(id = "FOSS4G_example", data = ourTask, target = "tsa")
# checking our data
head(mlr::getTaskData(ourTask))
```

### Defining our learners (learning algorithms)
```{r learnersMLR, exercise = TRUE}
# Defining our learners
ourLearners = mlr::makeLearners(
  cls = c("regr.lm", "regr.fnn", "regr.nnet"),
  ids = c("linearRegression", "Fast Nearest Neighbours", "Neural Network")
)
```

### Defining our resampling strategy
```{r resamplMLR, exercise = TRUE}
# Defining our learners
ourResamplingStrategy = mlr::makeResampleDesc("LOO")
```

### Performing our benchmark

> Let's find which learner provides the best results (the lowest RMSE) for our specific spatial interpolation problem

```{r bmrMLR, exercise = TRUE, message= FALSE}
# performing the benchmark of our learners on our task
ourbenchmark = mlr::benchmark(
  learners = ourLearners,
  tasks = ourTask,
  resamplings = ourResamplingStrategy,
  measures = list(rmse)
)

performances = mlr::getBMRAggrPerformances(bmr = ourbenchmark, as.df = TRUE)
performances

# Vizualizing the benchamrk result
library(ggplot2)
plotBMRBoxplots(
  bmr = ourbenchmark,
  measure = rmse,
  order.lrns = mlr::getBMRLearnerIds(ourbenchmark)) +
  aes(color = learner.id)
```

### Training the best learner

> let's train our best (lowest RMSE) learner (neural network) on our dataset

```{r trainMLR, exercise = TRUE, message= FALSE}
# extract the lon and lat from the sf geometry
coords = data.frame(st_coordinates(extraction))
# attributing our original dataset to another var (to avoid overwriting)
ourPredictionGrid = extraction
# converting our dataset from sf to simple df
st_geometry(ourPredictionGrid) <- NULL
# joining the coords
ourPredictionGrid = dplyr::bind_cols(ourPredictionGrid, coords)

# training the neural net on the dataset
ourModel = mlr::train(
  learner = mlr::getBMRLearners(bmr = ourbenchmark)[[1]],
  task = ourTask)
```

### Predicting using the trained learner

> Let's predict the value of tsa at the locations of our grid

```{r predictMLR, exercise = TRUE, message= FALSE}
# using our model to make the prediction
ourPrediction = predict(
  object = ourModel,
  newdata = ourPredictionGrid
)$data

# injecting the predicted values in the prediction grid
ourPredictedGrid = dplyr::bind_cols(ourPredictionGrid, ourPrediction)

# making the predicted grid a spatial object again
ourPredictedGrid = sf::st_as_sf(ourPredictedGrid, coords = c("X", "Y"), crs = 4326)
plot(ourPredictedGrid)

# Let's fake a raster rendering for better rendering
sfgrid = st_sf(sf::st_make_grid(x = sf::st_transform(wallonia, 3812),  cellsize = 5000, what = "polygons"))
ourPredictedGrid = sf::st_transform(ourPredictedGrid, crs = 3812)
ourPredictedGrid = sf::st_join(sfgrid, ourPredictedGrid)
ourPredictedGrid = ourPredictedGrid %>%
  dplyr::filter(!is.na(response))
```

### Mapping the prediction

> Adding our prediction layer to leaflet map

```{r mapMLR, exercise = TRUE, message= FALSE}
# reprojecting to 4326 for leaflet
ourPredictedGrid = sf::st_transform(ourPredictedGrid, 4326)

# adding to our map
map2 = map %>% 
  addPolygons(
    data = ourPredictedGrid,
    group = "Predictions",
    color = ~temperature.pal(response),
    stroke = FALSE,
    fillOpacity = 0.9,
    label = ~htmltools::htmlEscape(as.character(response))) %>%
  addLegend(
    values = ourPredictedGrid$response,
    group = "Predictions",
    position = "bottomleft", pal = temperature.pal,
    title = "predicted T (°C)") %>%
  addLayersControl(
         baseGroups = c("B&W", "Satelitte"),
         overlayGroups = c("Elevation", "Admin", "Grid", "Predictions", "Stations"),
         options = layersControlOptions(collapsed = TRUE)
     )
map2
```

## YOUR TURN !

### What is my spatial background ? 

* Master in Geography (2007) @ULB
* Few years of research : climate change, ice cores : no spatial skills
* 2015 : first coding experience (JS) : no spatial skills
* 2016 : stateOftheMap, foss4gbxl, opensource GIS : 
* 2017 : job as geostatistician @CRAW. First experience with R 
* 2018 : giving a first talk @foss4gbxl about spatial with R !
* __The point__ : don't be afraid to code. Yes, you can too !

### How do I get Started ? 

## Ressources

Check my [curated list of free tools and datasets on my blog](https://pokyah.github.io/geo-tools/). I highly recommad to start with the 2 following items :

* [datacamp course](https://www.datacamp.com/courses/spatial-analysis-in-r-with-sf-and-raster?tap_a=5644-dce66f&tap_s=10907-287229)
* [geocomputation with R](https://geocompr.robinlovelace.net)

## ABOUT

This presentation was built using `learnr`, a package to create your own interactive tutorials.

As mentioned on to deploy it on [shinyapps.io](https://shiny.rstudio.com/articles/shinyapps.html), you need to have devtools >= 1.4

Reproduciblility is only assured by providing complete setup instructions and resources. Docker is your best friend for this purpose.

* https://www.shinyproxy.io/deploying-apps/#run-shinyproxy
* https://github.com/o2r-project/containerit/blob/master/vignettes/containerit.Rmd
* https://www.shinyproxy.io/deploying-apps/
* https://jlintusaari.github.io/2018/07/how-to-compile-rmarkdown-documents-using-docker/


devtools::install_github("hadley/devtools")
devtools::install_github("rstudio/packrat")
devtools::install_github("tidyverse/dplyr")
library(devtools)

You might also be interested in :

* `thesisdown` : to author thesis with R + markdown
* `blogdown` to author your jekyll/hugo glog with R + markdown
* `bookdown` to author books with R + markdown
